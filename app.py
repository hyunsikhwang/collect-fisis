import streamlit as st
import aiohttp
import asyncio
import pandas as pd
import nest_asyncio
import json
import time
import duckdb
import os
import plotly.graph_objects as go
import plotly.express as px
import requests
from datetime import datetime

# Streamlit íŽ˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë³´í—˜ì‚¬ ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨ ìˆ˜ì§‘ê¸°",
    page_icon="ðŸ“Š",
    layout="wide"
)

# ë¹„ë™ê¸° ë£¨í”„ ì¶©ëŒ ë°©ì§€
nest_asyncio.apply()

# ==========================================
# 1. ìƒìˆ˜ ë° ê¸°ë³¸ ì„¤ì •
# ==========================================
# API í‚¤ (st.secrets ì²˜ë¦¬ í›„ í•„ìš”ì‹œ UIì—ì„œ ìž…ë ¥)
API_KEY = st.secrets.get("FSS_API_KEY", "")
TARGET_MONTH = "202509" # ê¸°ë³¸ê°’ ì„¤ì •

TERM = "Q" # ë¶„ê¸°
BASE_URL = "http://fisis.fss.or.kr/openapi"
MAX_CONCURRENT_REQUESTS = 20

# ==========================================
# 1.5. MotherDuck DB ì„¤ì •
# ==========================================
MD_TOKEN = st.secrets.get("MOTHERDUCK_TOKEN", "")
DB_NAME = "fisis_cache"
TABLE_NAME = "insurance_stats"
COLUMNS = ['êµ¬ë¶„', 'íšŒì‚¬ì½”ë“œ', 'íšŒì‚¬ëª…', 'ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…', 'ê¸°ì¤€ë…„ì›”', 'ë‹¨ìœ„', 'ê°’']

def get_md_connection():
    """MotherDuck ì—°ê²° ì„¤ì •"""
    if not MD_TOKEN:
        return None
    try:
        # MotherDuck ì—°ê²° (md: ë’¤ì— í† í°ì´ ì—†ìœ¼ë©´ st.secretsì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸)
        conn = duckdb.connect(f"md:?motherduck_token={MD_TOKEN}")
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ì‚¬ìš©
        conn.execute(f"CREATE DATABASE IF NOT EXISTS {DB_NAME}")
        conn.execute(f"USE {DB_NAME}")
        # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
        conn.execute(f"""
            CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
                êµ¬ë¶„ VARCHAR,
                íšŒì‚¬ì½”ë“œ VARCHAR,
                íšŒì‚¬ëª… VARCHAR,
                ê³„ì •ì½”ë“œ VARCHAR,
                ê³„ì •ëª… VARCHAR,
                ê¸°ì¤€ë…„ì›” VARCHAR,
                ë‹¨ìœ„ VARCHAR,
                ê°’ DOUBLE,
                ìˆ˜ì§‘ì¼ì‹œ TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        return conn
    except Exception as e:
        st.error(f"MotherDuck ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def get_cached_data(target_month):
    """MotherDuckì—ì„œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ"""
    conn = get_md_connection()
    if conn:
        try:
            df = conn.execute(f"SELECT * FROM {TABLE_NAME} WHERE ê¸°ì¤€ë…„ì›” = ?", [target_month]).df()
            conn.close()
            return df
        except Exception as e:
            st.warning(f"ë°ì´í„° ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    return pd.DataFrame()

def save_to_md(df):
    """ë°ì´í„°ë¥¼ MotherDuckì— ì €ìž¥"""
    if df.empty:
        return
    conn = get_md_connection()
    if conn:
        try:
            # ì»¬ëŸ¼ ìˆœì„œ ê³ ì • ë° ë°ì´í„° í´ë¦¬ë‹
            df_to_save = df[COLUMNS].copy()
            for col in ['íšŒì‚¬ì½”ë“œ', 'ê³„ì •ì½”ë“œ', 'ê¸°ì¤€ë…„ì›”']:
                df_to_save[col] = df_to_save[col].astype(str).str.strip()

            # ìž„ì‹œ ë·°ë¥¼ ìƒì„±í•˜ì—¬ ë°ì´í„°ë¥¼ ì ìž¬
            conn.register("df_to_save", df_to_save)
            # ëª…ì‹œì ìœ¼ë¡œ ì»¬ëŸ¼ì„ ì§€ì •í•˜ì—¬ INSERT (ìˆœì„œ ì¼ê´€ì„± ë³´ìž¥)
            col_names = ", ".join(COLUMNS) + ", ìˆ˜ì§‘ì¼ì‹œ"
            conn.execute(f"INSERT INTO {TABLE_NAME} ({col_names}) SELECT *, CURRENT_TIMESTAMP FROM df_to_save")
            conn.close()
        except Exception as e:
            st.error(f"ë°ì´í„° ì €ìž¥ ì‹¤íŒ¨: {e}")

def load_kics_analysis_data():
    """K-ICS ë¶„ì„ì„ ìœ„í•œ ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ê³„ì‚°"""
    conn = get_md_connection()
    if not conn:
        return pd.DataFrame()
    
    try:
        # ê´€ì‹¬ ìžˆëŠ” ê³„ì •ë“¤ë§Œ í•„í„°ë§í•´ì„œ ê°€ì ¸ì˜¤ê¸°
        target_accounts = [
            'ì§€ê¸‰ì—¬ë ¥ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© ì „)', 
            'ì§€ê¸‰ì—¬ë ¥ê¸°ì¤€ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© ì „)',
            'ì§€ê¸‰ì—¬ë ¥ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© í›„)', 
            'ì§€ê¸‰ì—¬ë ¥ê¸°ì¤€ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© í›„)'
        ]
        
        # [DEBUG] ë””ë²„ê¹… ì˜µì…˜ (Dashboard ìƒë‹¨ì— í‘œì‹œë¨)
        show_debug = st.checkbox("ðŸ” ìƒì„¸ ë°ì´í„° ì¶”ì¶œ ê³¼ì • í™•ì¸ (ë””ë²„ê±°)", value=False)
        
        # 1. DBì— ìžˆëŠ” ëª¨ë“  ë…íŠ¹í•œ ê³„ì •ëª… í™•ì¸
        all_accounts = conn.execute(f"SELECT DISTINCT ê³„ì •ëª… FROM {TABLE_NAME}").df()['ê³„ì •ëª…'].tolist()
        if show_debug:
            st.write(f"DEBUG: DB ë‚´ ì´ ê³„ì • ìˆ˜: {len(all_accounts)}")
            st.write(f"DEBUG: DB ë‚´ ê³„ì • ìƒ˜í”Œ: {all_accounts[:5]}")
        
        # 2. ìœ ì‚¬í•œ ê³„ì •ëª… ë§¤í•‘ (ê³µë°± ì œê±° ë° ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰ìœ¼ë¡œ ê°•í™”)
        def find_best_match(target, candidates):
            target_clean = target.replace(" ", "")
            # ì™„ì „ ì¼ì¹˜(ê³µë°± ì œê±°)
            for c in candidates:
                if c.replace(" ", "") == target_clean:
                    return c
            # ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰
            for c in candidates:
                if target_clean in c.replace(" ", "") or c.replace(" ", "") in target_clean:
                    return c
            return target

        actual_targets = [find_best_match(t, all_accounts) for t in target_accounts]
        if show_debug:
            st.write(f"DEBUG: ë§¤í•‘ëœ íƒ€ê²Ÿ ê³„ì •: {actual_targets}")
        
        # IN ì ˆ íŒŒë¼ë¯¸í„° ìƒì„±
        placeholders = ', '.join(['?' for _ in actual_targets])
        query = f"SELECT * FROM {TABLE_NAME} WHERE ê³„ì •ëª… IN ({placeholders})"
        df = conn.execute(query, actual_targets).df()
        conn.close()
        
        if show_debug:
            st.write(f"DEBUG: ì¡°íšŒëœ ë¡œìš° ìˆ˜: {len(df)}")

        if df.empty:
            return pd.DataFrame()

        # ë°ì´í„° í´ë¦¬ë‹
        df['ê¸°ì¤€ë…„ì›”'] = df['ê¸°ì¤€ë…„ì›”'].astype(str).str.strip()
        
        # ë§¤í•‘ìš© ì‚¬ì „ ìƒì„± (ì›ëž˜ ì´ë¦„ìœ¼ë¡œ í†µì¼)
        name_map = dict(zip(actual_targets, target_accounts))
        df['ê³„ì •ëª…'] = df['ê³„ì •ëª…'].map(name_map)
        
        if show_debug:
            st.write("DEBUG: ê³„ì •ëª… ë§¤í•‘ í›„ ë°ì´í„° ìƒ˜í”Œ:", df.head())

        # í”¼ë²—í•˜ì—¬ ê³„ì‚°í•˜ê¸° ì‰½ê²Œ ë³€í™˜
        # ê³„ì •ëª…ì´ ì¤‘ë³µë  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ (ë™ì¼ íšŒì‚¬ê°€ ê°™ì€ ë‹¬ì— ì—¬ëŸ¬ë²ˆ ìˆ˜ì§‘ëœ ê²½ìš° ë“±) sumìœ¼ë¡œ ì§‘ê³„
        pdf = df.pivot_table(
            index=['êµ¬ë¶„', 'ê¸°ì¤€ë…„ì›”', 'íšŒì‚¬ëª…'],
            columns='ê³„ì •ëª…',
            values='ê°’',
            aggfunc='sum'
        ).reset_index()
        
        if show_debug:
            st.write("DEBUG: í”¼ë²— í›„ ë°ì´í„° ì»¬ëŸ¼:", pdf.columns.tolist())
            st.write("DEBUG: í”¼ë²— í›„ ë°ì´í„° ìˆ˜:", len(pdf))
        pdf = df.pivot_table(
            index=['êµ¬ë¶„', 'ê¸°ì¤€ë…„ì›”', 'íšŒì‚¬ëª…'],
            columns='ê³„ì •ëª…',
            values='ê°’',
            aggfunc='sum'
        ).reset_index()
        
        # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìžˆëŠ”ì§€ í™•ì¸ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€)
        for col in target_accounts:
            if col not in pdf.columns:
                pdf[col] = 0

        # ê·¸ë£¹ë³„ í•©ê³„ ê³„ì‚° (ìƒëª…ë³´í—˜, ì†í•´ë³´í—˜, ì „ì²´)
        # 1. ìƒëª…/ì†í•´ë³„ í•©ê³„
        grouped = pdf.groupby(['êµ¬ë¶„', 'ê¸°ì¤€ë…„ì›”'])[target_accounts].sum().reset_index()
        
        # 2. ì „ì²´(Total) í•©ê³„ ìƒì„±
        total = pdf.groupby(['ê¸°ì¤€ë…„ì›”'])[target_accounts].sum().reset_index()
        total['êµ¬ë¶„'] = 'ì „ì²´'
        
        # ê²°í•©
        final_df = pd.concat([grouped, total], ignore_index=True)
        
        # K-ICS ë¹„ìœ¨ ê³„ì‚° (%)
        # ê²½ê³¼ì¡°ì¹˜ ì „
        final_df['ratio_before'] = (final_df['ì§€ê¸‰ì—¬ë ¥ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© ì „)'] / 
                                    final_df['ì§€ê¸‰ì—¬ë ¥ê¸°ì¤€ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© ì „)'].replace(0, pd.NA)) * 100
        # ê²½ê³¼ì¡°ì¹˜ í›„
        final_df['ratio_after'] = (final_df['ì§€ê¸‰ì—¬ë ¥ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© í›„)'] / 
                                   final_df['ì§€ê¸‰ì—¬ë ¥ê¸°ì¤€ê¸ˆì•¡(ê²½ê³¼ì¡°ì¹˜ ì ìš© í›„)'].replace(0, pd.NA)) * 100
        
        # ì •ë ¬ (ë‚ ì§œìˆœ)
        final_df = final_df.sort_values('ê¸°ì¤€ë…„ì›”')
        
        return final_df
    except Exception as e:
        st.error(f"ë¶„ì„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def fetch_ecos_bond_yield(start_month, end_month):
    """ECOSì—ì„œ êµ­ê³ ì±„ 10ë…„ ê¸ˆë¦¬ ì¡°íšŒ"""
    ECOS_API_KEY = st.secrets.get("ECOS_API_KEY", "")
    if not ECOS_API_KEY:
        return pd.DataFrame()
    
    # K-ICS ë°ì´í„° ë²”ìœ„ì— ë§žì¶° ì‹œìž‘/ì¢…ë£Œì¼ ì„¤ì •
    # start_month/end_month: '202303' í˜•ì‹ -> '20230301' / '20230331' ë“±ìœ¼ë¡œ ë³€í™˜ í•„ìš”í•˜ë‚˜
    # ECOSëŠ” ë‹¨ìˆœížˆ ì•žë’¤ ë‚ ì§œë§Œ ë„‰ë„‰ížˆ ì£¼ë©´ ë¨
    start_date = f"{start_month}01"
    # í˜„ìž¬ ë‚ ì§œ ê¸°ì¤€
    KST = timezone('Asia/Seoul')
    nowSeo = datetime.now(KST).strftime('%Y%m%d')
    
    bond_cd = '010210000' # êµ­ê³ ì±„ 10ë…„
    url = f'http://ecos.bok.or.kr/api/StatisticSearch/{ECOS_API_KEY}/json/kr/1/10000/817Y002/D/{start_date}/{nowSeo}/{bond_cd}'

    try:
        res = requests.get(url, timeout=10)
        data = res.json()
        if 'StatisticSearch' in data and 'row' in data['StatisticSearch']:
            rows = data['StatisticSearch']['row']
            df = pd.DataFrame(rows)
            df['yield'] = df['DATA_VALUE'].astype(float)
            # TIME: 20230301 -> ê¸°ì¤€ë…„ì›” 202303 ì¶”ì¶œ
            df['ê¸°ì¤€ë…„ì›”'] = df['TIME'].str[:6]
            
            # ì›”ë³„ ë§ˆì§€ë§‰ ì˜ì—…ì¼ ê¸°ì¤€ ê¸ˆë¦¬ ì¶”ì¶œ (K-ICS ëŒ€ë¹„ìš©)
            df_monthly = df.groupby('ê¸°ì¤€ë…„ì›”').last().reset_index()[['ê¸°ì¤€ë…„ì›”', 'yield']]
            return df_monthly
    except Exception as e:
        st.warning(f"ECOS ê¸ˆë¦¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return pd.DataFrame()

# ==========================================
# 2. ë¹„ë™ê¸° í†µì‹  í•¨ìˆ˜ ì •ì˜
# ==========================================
async def fetch_json(session, url, params):
    try:
        async with session.get(url, params=params, timeout=10) as response:
            if response.status == 200:
                text = await response.text()
                try:
                    return json.loads(text)
                except json.JSONDecodeError:
                    return None
            else:
                return None
    except Exception:
        return None

async def get_companies(session, part_div):
    """ê¸ˆìœµíšŒì‚¬ ì½”ë“œ ì¡°íšŒ"""
    url = f"{BASE_URL}/companySearch.json"
    params = {"lang": "kr", "auth": API_KEY, "partDiv": part_div}
    data = await fetch_json(session, url, params)

    company_list = []
    if data and 'result' in data and 'list' in data['result']:
        for item in data['result']['list']:
            company_list.append({
                'financeCd': item['finance_cd'],
                'financeNm': item['finance_nm'],
                'partDiv': part_div
            })
    return company_list

async def get_accounts(session, list_no):
    """ê³„ì •í•­ëª© ì¡°íšŒ"""
    url = f"{BASE_URL}/accountListSearch.json"
    params = {"lang": "kr", "auth": API_KEY, "listNo": list_no}
    data = await fetch_json(session, url, params)

    account_list = []
    if data and 'result' in data and 'list' in data['result']:
        for item in data['result']['list']:
            account_list.append({
                'accountCd': item['account_cd'],
                'accountNm': item['account_nm'],
                'listNo': list_no
            })
    return account_list

async def fetch_statistics(session, semaphore, company, account, pbar, status_text):
    """í†µê³„ì •ë³´ ìˆ˜ì§‘"""
    url = f"{BASE_URL}/statisticsInfoSearch.json"
    params = {
        "lang": "kr",
        "auth": API_KEY,
        "financeCd": company['financeCd'],
        "listNo": account['listNo'],
        "accountCd": account['accountCd'],
        "term": TERM,
        "startBaseMm": TARGET_MONTH,
        "endBaseMm": TARGET_MONTH
    }

    async with semaphore:
        data = await fetch_json(session, url, params)
    
    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (UI) - ë„ˆë¬´ ìž¦ì€ ì—…ë°ì´íŠ¸ëŠ” ì„±ëŠ¥ ì €í•˜ë¥¼ ìœ ë°œí•˜ë¯€ë¡œ ì£¼ì˜
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨ížˆ ë¡œì§ë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜

    if data and 'result' in data and 'list' in data['result']:
        result_list = data['result']['list']
        if result_list:
            item = result_list[0]
            # ê°’ ìš°ì„ ìˆœìœ„ í™•ì¸
            raw_value = item.get('a') or item.get('won') or item.get('column_value') or 0

            return {
                'êµ¬ë¶„': 'ìƒëª…ë³´í—˜' if company['partDiv'] == 'H' else 'ì†í•´ë³´í—˜',
                'íšŒì‚¬ì½”ë“œ': company['financeCd'],
                'íšŒì‚¬ëª…': company['financeNm'],
                'ê³„ì •ì½”ë“œ': account['accountCd'],
                'ê³„ì •ëª…': account['accountNm'],
                'ê¸°ì¤€ë…„ì›”': TARGET_MONTH, # API ê²°ê³¼ì™€ ìƒê´€ì—†ì´ ìš”ì²­í•œ ê¸°ì¤€ë…„ì›”ë¡œ ì €ìž¥ (ì¼ê´€ì„± ìœ ì§€)
                'ë‹¨ìœ„': item.get('unit_name', ''),
                'ê°’': raw_value
            }
    return None

# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ (Async Wrapper)
# ==========================================
async def run_async_collection():
    status_container = st.status("ðŸš€ ë°ì´í„° ìˆ˜ì§‘ ë° ìºì‹œ í™•ì¸ ì¤‘...", expanded=True)
    
    try:
        # 0. MotherDuck ìºì‹œ í™•ì¸
        status_container.write(f"ðŸ”Ž {TARGET_MONTH} ë°ì´í„° ìºì‹œ í™•ì¸ ì¤‘...")
        cached_df = get_cached_data(TARGET_MONTH)
        
        if not cached_df.empty:
            status_container.write(f"âœ… {len(cached_df)}ê±´ì˜ ë°ì´í„°ë¥¼ MotherDuckì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            status_container.write("â„¹ï¸ í•´ë‹¹ ì›”ì˜ ìºì‹œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        async with aiohttp.ClientSession() as session:
            # 1. ëª©ë¡ ì¡°íšŒ
            status_container.write("ðŸ” 1. ê¸ˆìœµíšŒì‚¬ ë° ê³„ì •í•­ëª© ëª©ë¡ ì¡°íšŒ ì¤‘...")
            
            # ë³‘ë ¬ë¡œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            f1 = get_companies(session, 'H')
            f2 = get_companies(session, 'I')
            f3 = get_accounts(session, 'SH021')
            f4 = get_accounts(session, 'SI021')
            
            life_companies, non_life_companies, life_accounts, non_life_accounts = await asyncio.gather(f1, f2, f3, f4)
            
            total_companies = len(life_companies) + len(non_life_companies)
            status_container.write(f"âœ… íšŒì‚¬ ëª©ë¡ í™•ë³´: ì´ {total_companies}ê°œ")

            # 2. ìž‘ì—… ìƒì„± (ìºì‹œì— ì—†ëŠ” ê²ƒë§Œ)
            tasks = []
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
            
            status_container.write("ðŸ“¦ 2. ë¯¸ìˆ˜ì§‘ ë°ì´í„° í™•ì¸ ë° ìš”ì²­ ìƒì„± ì¤‘...")
            # ê¸°ì¡´ ë°ì´í„° í‚¤ ìƒì„± (íšŒì‚¬ì½”ë“œ, ê³„ì •ì½”ë“œ)
            existing_keys = set()
            if not cached_df.empty:
                # ë°ì´í„° íƒ€ìž…ì„ ë¬¸ìžì—´ë¡œ ê°•ì œ ë³€í™˜ ë° ê³µë°± ì œê±° (ìºì‹œ ë¯¸ìŠ¤ ë°©ì§€)
                existing_keys = set(zip(
                    cached_df['íšŒì‚¬ì½”ë“œ'].astype(str).str.strip(), 
                    cached_df['ê³„ì •ì½”ë“œ'].astype(str).str.strip()
                ))

            def build_tasks(companies, accounts):
                for comp in companies:
                    for acc in accounts:
                        # ë¹„êµ ì‹œì—ë„ ë¬¸ìžì—´ë¡œ ë³€í™˜ ë° ê³µë°± ì œê±°
                        f_cd = str(comp['financeCd']).strip()
                        a_cd = str(acc['accountCd']).strip()
                        if (f_cd, a_cd) not in existing_keys:
                            tasks.append(fetch_statistics(session, semaphore, comp, acc, None, None))

            build_tasks(life_companies, life_accounts)
            build_tasks(non_life_companies, non_life_accounts)

            total_tasks = len(tasks)
            
            if total_tasks == 0:
                status_container.write("âœ¨ ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ìºì‹œë˜ì–´ ìžˆìŠµë‹ˆë‹¤.")
                status_container.update(label="âœ… ìºì‹œ ë°ì´í„° ë¦¬ë¡œë“œ ì™„ë£Œ!", state="complete", expanded=False)
                return cached_df.to_dict('records')

            status_container.write(f"ðŸ“¡ {len(existing_keys)}ê±´ì€ ìºì‹œì—ì„œ ë°œê²¬í–ˆê³ , {total_tasks} ê±´ì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ APIë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")

            # 3. ì‹¤í–‰ ë° ì§„í–‰ë¥  í‘œì‹œ
            new_results = []
            progress_bar = status_container.progress(0)
            completed_count = 0
            
            for f in asyncio.as_completed(tasks):
                res = await f
                if res:
                    new_results.append(res)
                
                completed_count += 1
                if total_tasks > 0:
                    progress_bar.progress(completed_count / total_tasks)

            # 4. ìƒˆë¡œìš´ ë°ì´í„° DB ì €ìž¥
            if new_results:
                status_container.write(f"ðŸ’¾ {len(new_results)}ê±´ì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ MotherDuckì— ì €ìž¥ ì¤‘...")
                new_df = pd.DataFrame(new_results)
                # ê°’ ì „ì²˜ë¦¬ (ì €ìž¥ ì „ ìˆ«ìžë¡œ ë³€í™˜)
                new_df['ê°’'] = pd.to_numeric(new_df['ê°’'].astype(str).str.replace(',', ''), errors='coerce')
                save_to_md(new_df)
                
                # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
                if not cached_df.empty:
                    # ì»¬ëŸ¼ ìˆœì„œ ë° ì´ë¦„ ì¼ê´€ì„± í™•ë³´
                    all_results_df = pd.concat([cached_df[COLUMNS], new_df[COLUMNS]], ignore_index=True)
                    results = all_results_df.to_dict('records')
                else:
                    results = new_results
            else:
                results = cached_df.to_dict('records')

            status_container.update(label="âœ… ë°ì´í„° ìˆ˜ì§‘ ë° ìºì‹± ì™„ë£Œ!", state="complete", expanded=False)
            return results

    except Exception as e:
        status_container.update(label="âš ï¸ ì˜¤ë¥˜ ë°œìƒ", state="error")
        st.error(f"ì˜¤ë¥˜ ìƒì„¸: {e}")
        return []

# ==========================================
# 4. Streamlit UI êµ¬ì„±
# ==========================================
st.title("ðŸ“Š ë³´í—˜ì‚¬ ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# ë©”ì¸ íƒ­ ë¶„ë¦¬: ë¶„ì„ ëŒ€ì‹œë³´ë“œì™€ ë°ì´í„° ìˆ˜ì§‘ê¸°
main_tab1, main_tab2 = st.tabs(["ðŸ“ˆ ë¶„ì„ ëŒ€ì‹œë³´ë“œ (Dashboard)", "ðŸ“¡ ë°ì´í„° ìˆ˜ì§‘ê¸° (Collector)"])

with main_tab1:
    st.subheader("ðŸ“Š K-ICS ë¹„ìœ¨ ì¶”ì´ ë¶„ì„")
    st.info("MotherDuckì— ì €ìž¥ëœ ëª¨ë“  ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œê³„ì—´ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    
    analysis_df = load_kics_analysis_data()
    
    if not analysis_df.empty:
        # Plotly ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()
        
        # ìƒ‰ìƒ ë° ìŠ¤íƒ€ì¼ ì„¤ì •
        styles = {
            'ìƒëª…ë³´í—˜': {'color': '#1f77b4'},
            'ì†í•´ë³´í—˜': {'color': '#ff7f0e'},
            'ì „ì²´': {'color': '#2ca02c'}
        }
        
        for g in ['ìƒëª…ë³´í—˜', 'ì†í•´ë³´í—˜', 'ì „ì²´']:
            g_df = analysis_df[analysis_df['êµ¬ë¶„'] == g]
            
            # ê²½ê³¼ì¡°ì¹˜ ì ìš© ì „ (ì ì„ )
            fig.add_trace(go.Scatter(
                x=g_df['ê¸°ì¤€ë…„ì›”'], 
                y=g_df['ratio_before'],
                name=f"{g} (ê²½ê³¼ì¡°ì¹˜ ì „)",
                line=dict(color=styles[g]['color'], dash='dot', width=2),
                mode='markers+lines',
                marker=dict(size=8)
            ))
            
            # ê²½ê³¼ì¡°ì¹˜ ì ìš© í›„ (ì‹¤ì„ )
            fig.add_trace(go.Scatter(
                x=g_df['ê¸°ì¤€ë…„ì›”'], 
                y=g_df['ratio_after'],
                name=f"{g} (ê²½ê³¼ì¡°ì¹˜ í›„)",
                line=dict(color=styles[g]['color'], width=4),
                mode='markers+lines',
                marker=dict(size=10)
            ))
        
        fig.update_layout(
            title="ë³´í—˜ì—…ê¶Œë³„ K-ICS ë¹„ìœ¨ ë° êµ­ê³ ì±„ 10ë…„ ê¸ˆë¦¬ ì¶”ì´",
            xaxis_title="ê¸°ì¤€ë…„ì›”",
            yaxis_title="K-ICS Ratio (%)",
            yaxis2=dict(
                title="êµ­ê³ ì±„ 10ë…„ ê¸ˆë¦¬ (%)",
                overlaying='y',
                side='right',
                showgrid=False
            ),
            legend_title="êµ¬ë¶„",
            template="plotly_white",
            hovermode="x unified",
            height=600,
            yaxis=dict(ticksuffix="%")
        )

        # ECOS ê¸ˆë¦¬ ë°ì´í„° ì¶”ê°€
        min_month = analysis_df['ê¸°ì¤€ë…„ì›”'].min()
        max_month = analysis_df['ê¸°ì¤€ë…„ì›”'].max()
        
        bond_df = fetch_ecos_bond_yield(min_month, max_month)
        
        if not bond_df.empty:
            # ì‹œê°í™” ê¸°ê°„ì— ë§žê²Œ í•„í„°ë§
            bond_df = bond_df[(bond_df['ê¸°ì¤€ë…„ì›”'] >= min_month) & (bond_df['ê¸°ì¤€ë…„ì›”'] <= max_month)]
            
            fig.add_trace(go.Scatter(
                x=bond_df['ê¸°ì¤€ë…„ì›”'],
                y=bond_df['yield'],
                name="êµ­ê³ ì±„ 10ë…„ (ìš°ì¶•)",
                line=dict(color='gray', width=3, dash='dash'),
                yaxis='y2',
                mode='lines+markers',
                marker=dict(symbol='diamond', size=10)
            ))
        else:
            if not st.secrets.get("ECOS_API_KEY"):
                st.caption("â„¹ï¸ ECOS_API_KEYë¥¼ ì„¤ì •í•˜ë©´ êµ­ê³ ì±„ ê¸ˆë¦¬ë¥¼ í•¨ê»˜ ë³´ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ë¶„ì„ ë°ì´í„° í…Œì´ë¸”
        with st.expander("ðŸ“ ìƒì„¸ ìˆ˜ì¹˜ ë°ì´í„° í™•ì¸"):
            st.dataframe(analysis_df, use_container_width=True)
    else:
        st.warning("í‘œì‹œí•  ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ë°ì´í„° ìˆ˜ì§‘ê¸°' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ ì£¼ì„¸ìš”.")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë°ì´í„° í˜„í™© ì„¸ì…˜ (Dashboardì—ì„œë„ ë°ì´í„°ê°€ ì—†ì„ ë•Œ í‘œì‹œ)
        with st.expander("ðŸ› ï¸ ë°ì´í„°ë² ì´ìŠ¤ í˜„í™© í™•ì¸ (ë””ë²„ê¹…)"):
            conn = get_md_connection()
            if conn:
                try:
                    count = conn.execute(f"SELECT COUNT(*) FROM {TABLE_NAME}").fetchone()[0]
                    st.write(f"í˜„ìž¬ ì´ ë ˆì½”ë“œ ìˆ˜: {count}ê±´")
                    
                    st.write("ë³´ê´€ ì¤‘ì¸ ê³„ì •ëª… ëª©ë¡:")
                    distinct_accounts = conn.execute(f"SELECT DISTINCT ê³„ì •ëª… FROM {TABLE_NAME}").df()
                    st.dataframe(distinct_accounts)
                    
                    st.write("ë³´ê´€ ì¤‘ì¸ ê¸°ì¤€ë…„ì›” ëª©ë¡:")
                    distinct_months = conn.execute(f"SELECT DISTINCT ê¸°ì¤€ë…„ì›” FROM {TABLE_NAME} ORDER BY ê¸°ì¤€ë…„ì›”").df()
                    st.dataframe(distinct_months)
                    
                    conn.close()
                except Exception as e:
                    st.error(f"í˜„í™© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            else:
                st.warning("MotherDuck ì—°ê²° ì‹¤íŒ¨ (í† í° í™•ì¸ í•„ìš”)")

with main_tab2:
    st.subheader("ðŸ“¡ FSS Open API ë°ì´í„° ìˆ˜ì§‘")
    
    # ì„¤ì • ì„¹ì…˜ (ê¸°ì¡´ ì‚¬ì´ë“œë°”ì—ì„œ ì´ë™)
    with st.expander("âš™ï¸ ìˆ˜ì§‘ ì„¤ì • (Settings)", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            if not st.secrets.get("FSS_API_KEY"):
                API_KEY = st.text_input(
                    "ê¸ˆìœµê°ë…ì› API Key", 
                    value=API_KEY,
                    type="password",
                    help="ì¸ì¦í‚¤ë¥¼ ìž…ë ¥í•˜ì„¸ìš”."
                )
            else:
                st.success("âœ… API Keyê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                API_KEY = st.secrets.get("FSS_API_KEY")
        
        with col2:
            TARGET_MONTH = st.text_input(
                "ìˆ˜ì§‘ ê¸°ì¤€ë…„ì›” (YYYYMM)", 
                value="202509",
                help="ì¡°íšŒí•˜ê³  ì‹¶ì€ ë…„ì›”ì„ ìž…ë ¥í•˜ì„¸ìš”."
            )

    st.markdown(f"""
    Open APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´í—˜ì‚¬ì˜ ì§€ê¸‰ì—¬ë ¥ë¹„ìœ¨ ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  MotherDuckì— ì €ìž¥í•©ë‹ˆë‹¤.
    - **ëŒ€ìƒ**: ìƒëª…ë³´í—˜(H), ì†í•´ë³´í—˜(I)
    """)
    
    # ì‹¤í–‰ ë²„íŠ¼
    if st.button("ðŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œìž‘ (Start Collection)", type="primary"):
        if not API_KEY:
            st.error("API Keyë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”. (ì‚¬ì´ë“œë°”ì—ì„œ ìž…ë ¥ ê°€ëŠ¥)")
        else:
            # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
            raw_data = asyncio.run(run_async_collection())

            if raw_data:
                df = pd.DataFrame(raw_data)
                
                # ì „ì²˜ë¦¬
                df['ê°’'] = pd.to_numeric(df['ê°’'].astype(str).str.replace(',', ''), errors='coerce')

                # í”¼ë²— í…Œì´ë¸”
                df_pivot = df.pivot_table(
                    index=['êµ¬ë¶„', 'íšŒì‚¬ëª…', 'ê¸°ì¤€ë…„ì›”'],
                    columns='ê³„ì •ëª…',
                    values='ê°’',
                    aggfunc='first'
                ).reset_index()

                # ê²°ê³¼ ì„¹ì…˜
                st.divider()
                st.success(f"âœ… {TARGET_MONTH} ë°ì´í„° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                tab_res1, tab_res2 = st.tabs(["ðŸ“‹ ìš”ì•½ í…Œì´ë¸” (Pivot)", "ðŸ“„ RAW ë°ì´í„°"])
                
                with tab_res1:
                    st.subheader(f"{TARGET_MONTH} ìˆ˜ì§‘ ê²°ê³¼ (ìš”ì•½)")
                    st.dataframe(df_pivot, use_container_width=True)
                    
                    # CSV ë‹¤ìš´ë¡œë“œ
                    csv = df_pivot.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ðŸ’¾ ìˆ˜ì§‘ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                        data=csv,
                        file_name=f"insurance_solvency_{TARGET_MONTH}_result.csv",
                        mime="text/csv"
                    )

                with tab_res2:
                    st.subheader(f"{TARGET_MONTH} RAW ë°ì´í„°")
                    st.dataframe(df, use_container_width=True)
                
                # ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ í™”ë©´ ê°±ì‹ ì„ ìœ ë„í•˜ê±°ë‚˜ ì •ë³´ë¥¼ ì œê³µ
                st.info("ðŸ’¡ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤. 'ë¶„ì„ ëŒ€ì‹œë³´ë“œ' íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì°¨íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.")
            else:
                st.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. API Keyë‚˜ ê¸°ì¤€ë…„ì›”ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")